# **LLaMA and Mistral Model Integration README**

## **Overview**
This project demonstrates how to integrate and interact with two powerful AI models: the **LLaMA** model (Large Language Model by Meta) and **Mistral**, an advanced language model developed by Hugging Face. Both models can be used to generate AI-driven responses based on a given prompt, and the application is designed to serve as a basis for building AI-powered systems such as chatbots, virtual assistants, or automated content generation tools.

### **Key Features:**
- **LLaMA Model**: A family of foundational language models designed to handle complex language tasks with fine-tuned control over responses.
- **Mistral Model**: A high-performance model trained to deliver responses efficiently using the latest advancements in language model architecture.

## **Technologies Used**
- **Python 3.x**: The main programming language used to implement the system.
- **Flask**: A micro web framework used to create the API for interacting with the models.
- **Hugging Face Transformers**: A library for working with transformer-based models like Mistral and LLaMA.
- **FAISS**: A library for efficient similarity search and clustering, used here to index and retrieve relevant document chunks.

## **Setup & Installation**

### 1. **Clone the Repository**
   Clone the repository to your local machine:

   ```bash
   git clone <repository-url>
   cd <repository-directory>
   ```

### 2. **Create and Activate a Virtual Environment**
   It's recommended to use a virtual environment to manage your dependencies.

   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows, use venv\Scripts\activate
   ```

### 3. **Install Dependencies**
   Install the required libraries using `pip`:

   ```bash
   pip install -r requirements.txt
   ```

   The `requirements.txt` should contain dependencies like:
   ```text
   Flask==2.x
   requests==2.x
   sentence-transformers==2.x
   faiss-cpu==1.7.2
   transformers==4.x
   ```

### 4. **Configure the Environment**

   - Make sure to set up any environment variables required for your project. For example, you might need an API key for external services or model endpoints. You can set these in `.env` or export them directly.

   Example for Flask setup:
   ```bash
   export FLASK_APP=app.py
   export FLASK_ENV=development
   ```

### 5. **Initialize FAISS Index and Embedding Model**

   - The project requires a pre-built FAISS index (`faiss_index.idx`) and the `SentenceTransformer` model.
   - If you don't already have the FAISS index, follow the process for indexing your document chunks in the `faiss_index.py` file.

## **Running the Application**

To start the Flask server:

```bash
python app.py
```

The Flask app will start running on `http://localhost:5000` by default.

## **API Endpoints**

### 1. **`POST /query`**
   This endpoint allows you to send a query and receive a generated response from either LLaMA or Mistral.

   **Request:**
   - Method: `POST`
   - Content-Type: `application/json`
   - Body:
     ```json
     {
       "query": "What is the capital of France?"
     }
     ```

   **Response:**
   ```json
   {
     "response": "The capital of France is Paris."
   }
   ```

   **Error Handling:**
   - `400`: If no query is provided or the query is invalid.
   - `500`: Internal server error (e.g., database or model failure).

## **How It Works**

### **LLaMA Model (Meta)**
LLaMA (Large Language Model Meta AI) is a state-of-the-art foundational model capable of handling a variety of tasks like text generation, summarization, and translation. In this project, the LLaMA model serves as the backbone for generating responses from a system query. 

#### **Usage with LM Studio API**
In this project, the LLaMA model is accessed through the **LM Studio API**. The system sends a request to this API endpoint with a user query and receives the corresponding response generated by the model.

### **Mistral Model (Hugging Face)**
Mistral is a cutting-edge language model, available through Hugging Face's model hub, and is designed for fast, efficient, and accurate text generation. In this project, Mistral is used similarly to LLaMA, but with the advantage of its more optimized response times.

#### **Integration via `requests`**
Both LLaMA and Mistral are integrated using the `requests` library to send HTTP POST requests to the respective model API endpoints. The payload sent to the APIs includes:
- **Model name** (e.g., `"mistral-7b-instruct-v0.3:2"`)
- **Prompt** (including context and user query)
- **Other parameters** (like `max_tokens`, `temperature`)

### **FAISS Index and Query Handling**
   - When a user sends a query, the system first uses FAISS to retrieve the most relevant document chunks.
   - These chunks are then passed as context to the model, which generates a response based on both the query and the context.

### **Sentences Transformer for Embeddings**
   - The `SentenceTransformer` model is used to generate embeddings for the document chunks to be indexed with FAISS. These embeddings represent the semantic meaning of the documents, enabling efficient similarity search.

## **Error Handling and Debugging**
   The Flask application provides basic error handling:
   - **400 Bad Request**: Returned if the request body is invalid or the query is empty.
   - **500 Internal Server Error**: Catches any exceptions that occur while processing the request, including issues with FAISS, the model, or database.

To debug the server:
- Check the logs for detailed error messages.
- Ensure the model endpoints (LLaMA and Mistral) are active and responsive.

## **Model Performance**
- **LLaMA Model**: Great for handling complex language tasks with fine-tuned responses.
- **Mistral Model**: Optimized for faster and more efficient responses, suitable for real-time applications.

## **Limitations**
- The current implementation uses an in-memory FAISS index for testing purposes. For production, it's recommended to store the index on disk.
- Models are being queried from local API endpoints (e.g., LM Studio), so ensure that these services are correctly running.

## **Contributing**
Feel free to fork the repository and submit pull requests. Contributions are welcome, especially in improving model integrations, optimizing FAISS queries, or adding more models to the system.

## **License**
This project is licensed under the MIT License.
